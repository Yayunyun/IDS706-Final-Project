{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pyspark.pandas as ps\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import coo_matrix, hstack\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./clean_df/clean_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(['Unnamed: 0'],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna(subset=['overview','director','runtime','year']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = ['title','director','actor','overview','genres_list','key','country']\n",
    "data[text_data] = data[text_data].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"key\"] = data[\"key\"].str.encode('ascii', 'ignore').str.decode('ascii')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_dummy(col,num = None):\n",
    "    li = set()\n",
    "    for i in range(len(data[col])):\n",
    "        if num is None:\n",
    "            try:\n",
    "                num = len(data[col][i].split(','))\n",
    "            except:\n",
    "                print(data[col][i])\n",
    "        for act in data[col][i].split(',')[:num]:\n",
    "            li.add(act)\n",
    "    li = list(li)\n",
    "    for element in li:\n",
    "        data[element] = data[col].astype(str).str.contains(element, case=False).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "country\n",
      "director\n",
      "actor\n",
      "genres_list\n",
      "key\n"
     ]
    }
   ],
   "source": [
    "dum = ['country','director','actor','genres_list','key']\n",
    "for d in dum:\n",
    "    if d == 'actor':\n",
    "        to_dummy(d,num = 4)\n",
    "    else:\n",
    "        to_dummy(d)\n",
    "    print(d)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(['id','title','director','actor', 'overview','genres_list','key','country'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4770, 19229)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4770, 20876)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(stop_words = 'english')  # initialising the TF-IDF Vector object\n",
    "tfidf_matrix = tfidf.fit_transform(data['overview'])  # Constructing the TF-IDF Matrix (no. of movies x every word in vocabulary)\n",
    "tfidf_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = hstack([X,tfidf_matrix]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4770, 4770)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_sim = linear_kernel(X, X)  # Constructing the Cosine Similarity Matrix (no. of movies x no. of movies)\n",
    "cosine_sim.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.97855178e+04, -4.13424622e+00, -8.65318132e+00, ...,\n",
       "        -4.06511417e+01, -4.08878824e+01, -3.98424776e+01],\n",
       "       [-4.13424622e+00,  1.81518036e+04,  3.78242191e+02, ...,\n",
       "        -1.27782841e+01, -3.17877205e+01, -4.16209502e+01],\n",
       "       [-8.65318132e+00,  3.78242191e+02,  1.48416500e+04, ...,\n",
       "        -3.44993543e+01, -1.86323256e+01, -2.83943588e+01],\n",
       "       ...,\n",
       "       [-4.06511417e+01, -1.27782841e+01, -3.44993543e+01, ...,\n",
       "         2.07646290e+04, -2.27336759e+00, -1.12869813e+01],\n",
       "       [-4.08878824e+01, -3.17877205e+01, -1.86323256e+01, ...,\n",
       "        -2.27336759e+00,  8.04637577e+03,  7.23073814e+00],\n",
       "       [-3.98424776e+01, -4.16209502e+01, -2.83943588e+01, ...,\n",
       "        -1.12869813e+01,  7.23073814e+00,  9.97874978e+03]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cosine_sim.npy', 'wb') as f:\n",
    "    np.save(f, cosine_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cosine_sim.npy', 'rb') as f:\n",
    "    cosine_sim = np.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = pd.Series(data.index, index = data['title']).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that inputs movie titles and outputs top 10 movies similar to it\n",
    "\n",
    "def get_recommendations(title, cosine_sim = cosine_sim):\n",
    "  idx = indices[title]\n",
    "  \n",
    "  sim_scores = list(enumerate(cosine_sim[idx]))  # Get the similarity scores of all movies wrt input movie\n",
    "  sim_scores = sorted(sim_scores, key = lambda x : x[1], reverse = True)\n",
    "  sim_scores = sim_scores[1:11]\n",
    "  \n",
    "  movie_indices = [i[0] for i in sim_scores]\n",
    "  \n",
    "  return data['title'].iloc[movie_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65                     The Dark Knight\n",
       "119                      Batman Begins\n",
       "2207                         12 Rounds\n",
       "186                        Bad Boys II\n",
       "303                           Catwoman\n",
       "95                        Interstellar\n",
       "708     Maze Runner: The Scorch Trials\n",
       "2721                 Seven Psychopaths\n",
       "933                   Shanghai Knights\n",
       "96                           Inception\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_recommendations('The Dark Knight Rises')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_spark=spark.createDataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Word2Vec\n",
    "from pyspark.sql.functions import split\n",
    "from pyspark.sql.functions import lower, col\n",
    "data_spark = data_spark.withColumn(\"overview_splitted\", split(lower(col(\"overview\")), \" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2Vec = Word2Vec(vectorSize=100, minCount=0, maxIter=100, inputCol=\"overview_splitted\", outputCol=\"features\")\n",
    "model = word2Vec.fit(data_spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.transform(data_spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "assemble=VectorAssembler(inputCols=[\n",
    " 'popularity',\n",
    " 'vote_average',\n",
    " 'year'], outputCol='feature')\n",
    "assembled_data=assemble.transform(data_spark)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "849389c4839361bf72fb7e8d4d6756765439fa05b5bf20dcea1aa4759701de55"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
