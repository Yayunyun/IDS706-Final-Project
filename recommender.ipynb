{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pyspark.pandas as ps\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import coo_matrix, hstack\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"/workspaces/IDS706-Final-Project/clean_df/clean_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(['Unnamed: 0'],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna(subset=['overview','director','runtime','year']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = ['title','director','actor','overview','genres_list','key','country']\n",
    "data[text_data] = data[text_data].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"key\"] = data[\"key\"].str.encode('ascii', 'ignore').str.decode('ascii')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_dummy(col,num = None):\n",
    "    li = set()\n",
    "    for i in range(len(data[col])):\n",
    "        if num is None:\n",
    "            try:\n",
    "                num = len(data[col][i].split(','))\n",
    "            except:\n",
    "                print(data[col][i])\n",
    "        for act in data[col][i].split(',')[:num]:\n",
    "            li.add(act)\n",
    "    li = list(li)\n",
    "    for element in li:\n",
    "        data[element] = data[col].astype(str).str.contains(element, case=False).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dum = ['director','actor','genres_list','key','country']\n",
    "for d in dum:\n",
    "    if d == 'actor':\n",
    "        to_dummy(d,num = 4)\n",
    "    else:\n",
    "        to_dummy(d)\n",
    "    print(d)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(['id','title','director','actor', 'overview','genres_list','key','country'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4770, 19228)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4770, 20876)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(stop_words = 'english')  # initialising the TF-IDF Vector object\n",
    "tfidf_matrix = tfidf.fit_transform(data['overview'])  # Constructing the TF-IDF Matrix (no. of movies x every word in vocabulary)\n",
    "tfidf_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = hstack([X,tfidf_matrix]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4770, 4770)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_sim = linear_kernel(X, X)  # Constructing the Cosine Similarity Matrix (no. of movies x no. of movies)\n",
    "cosine_sim.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.52824867e+04,  1.61380201e+01,  4.54139529e+00, ...,\n",
       "        -3.05047409e+01, -3.30062328e+01, -3.09524484e+01],\n",
       "       [ 1.61380201e+01,  1.30060802e+04,  3.91278598e+02, ...,\n",
       "        -2.15051510e+01, -2.40526719e+01, -3.28775221e+01],\n",
       "       [ 4.54139529e+00,  3.91278598e+02,  1.21672372e+04, ...,\n",
       "        -3.15888136e+01, -1.79865360e+01, -2.67401897e+01],\n",
       "       ...,\n",
       "       [-3.05047409e+01, -2.15051510e+01, -3.15888136e+01, ...,\n",
       "         2.05681039e+04, -4.66418446e+00, -1.26694186e+01],\n",
       "       [-3.30062328e+01, -2.40526719e+01, -1.79865360e+01, ...,\n",
       "        -4.66418446e+00,  7.96679809e+03,  1.05214226e+01],\n",
       "       [-3.09524484e+01, -3.28775221e+01, -2.67401897e+01, ...,\n",
       "        -1.26694186e+01,  1.05214226e+01,  9.39743243e+03]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cosine_sim.npy', 'wb') as f:\n",
    "    np.save(f, cosine_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cosine_sim.npy', 'rb') as f:\n",
    "    cosine_sim = np.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = pd.Series(data.index, index = data['title']).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that inputs movie titles and outputs top 10 movies similar to it\n",
    "\n",
    "def get_recommendations(title, cosine_sim = cosine_sim):\n",
    "  idx = indices[title]\n",
    "  \n",
    "  sim_scores = list(enumerate(cosine_sim[idx]))  # Get the similarity scores of all movies wrt input movie\n",
    "  sim_scores = sorted(sim_scores, key = lambda x : x[1], reverse = True)\n",
    "  sim_scores = sim_scores[1:11]\n",
    "  \n",
    "  movie_indices = [i[0] for i in sim_scores]\n",
    "  \n",
    "  return data['title'].iloc[movie_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65                     The Dark Knight\n",
       "119                      Batman Begins\n",
       "2207                         12 Rounds\n",
       "186                        Bad Boys II\n",
       "95                        Interstellar\n",
       "303                           Catwoman\n",
       "708     Maze Runner: The Scorch Trials\n",
       "933                   Shanghai Knights\n",
       "2721                 Seven Psychopaths\n",
       "96                           Inception\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_recommendations('The Dark Knight Rises')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/13 22:49:54 WARN Utils: Your hostname, codespaces-fdee62 resolves to a loopback address: 127.0.0.1; using 172.16.5.4 instead (on interface eth0)\n",
      "22/12/13 22:49:54 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/13 22:49:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_spark=spark.createDataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Word2Vec\n",
    "from pyspark.sql.functions import split\n",
    "from pyspark.sql.functions import lower, col\n",
    "data_spark = data_spark.withColumn(\"overview_splitted\", split(lower(col(\"overview\")), \" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2Vec = Word2Vec(vectorSize=100, minCount=0, maxIter=100, inputCol=\"overview_splitted\", outputCol=\"features\")\n",
    "model = word2Vec.fit(data_spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.transform(data_spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "assemble=VectorAssembler(inputCols=[\n",
    " 'popularity',\n",
    " 'vote_average',\n",
    " 'year'], outputCol='feature')\n",
    "assembled_data=assemble.transform(data_spark)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "849389c4839361bf72fb7e8d4d6756765439fa05b5bf20dcea1aa4759701de55"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
